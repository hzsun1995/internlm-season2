# GPT4周年回望：大模型的再思考 - 林老师

- 框架的转换：Transformer开销巨大，目前moe是一种高效的稀疏模型，未来或许会有更多好的模型框架；（埋个坑：最近看Mamba比较多，感觉这种递归模型还是不能满足scaling laws）
- 训练数据：数据量需要跟参数量同步增长，数据质量和数据量对于LM非常重要；
- 多模态：万物皆可tokenize，但是这种方式或许不一定是最优选择。对于很多的模态来说，序列化会更有利于训练吗？
- 上下文：GPT-3-2k ; GPT4-32K ; GPT-4turbo - 128k ；-> 10M ;瓶颈 ： 长的上下文计算成本会非常高，同时目前长文本的机制并不具备对长文本压缩的能力，长文本内部的核心机制或许并不能被window所捕捉；
- agent：建立在LLM的指令跟随能力、反思能力、执行能力的基础之上，而非一个简单的流程化模式；
- 计算环境：算力拓展会慢慢演变为能源问题，大模型慢慢压缩大小也是一个趋势，1.8B模型也会展现出很好的能力，端侧的LM今年内或许会迎来技术爆发期，99%的推理由端侧LM完成，其他交给云侧；
- 如何评估国产LM在世界范围内的地位：opencompass - GPT4遥遥领先;国内的LLM普遍优于GPT3.5;要紧切关注弱于GPT-4的部分；
- 产业角度：国产模型已经可以作为产业应用，或许在某个垂直领域，如果训练数据足够优秀，是可以有机会超越GPT4的红线的；

- 建议：不是所有人、企业都适合卷LLM，要清楚自己的资源和优势，选择自己的差异化路径。
